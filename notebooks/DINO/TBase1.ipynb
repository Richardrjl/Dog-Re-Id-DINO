{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSBRXMpA5LYC"
   },
   "source": [
    "**Configuración de GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3785,
     "status": "ok",
     "timestamp": 1762971905884,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "8b7d8272",
    "outputId": "2be0c539-e0ce-4876-c63e-04045cddfc1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de PyTorch: 2.8.0+cu126\n",
      "CUDA disponible: True\n",
      "Cantidad de GPUs disponibles: 1\n",
      "Nombre de la GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verificar si PyTorch está usando la GPU\n",
    "print(\"Versión de PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Cantidad de GPUs disponibles:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "    # Configurar para usar la GPU de manera eficiente\n",
    "    # PyTorch usa la GPU por defecto si está disponible.\n",
    "    # Puedes mover tensores y modelos a la GPU explícitamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euL98_tmAWfB"
   },
   "source": [
    "**Extracción de Dataset en Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 375355,
     "status": "ok",
     "timestamp": 1762972281240,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "X1FYy8zUfAVW",
    "outputId": "d37df4c0-753a-4aff-bad9-966c6d30058d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos extraídos en: /content/\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "#Ruta al archivo comprimido\n",
    "dog_tar_path = '/content/drive/MyDrive/PetFace/images/dog.tar.gz'\n",
    "\n",
    "#Carpeta temporal para extraer\n",
    "extract_path = '/content/'\n",
    "\n",
    "#Crea la carpeta si no existe\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "#Extraer de forma segura\n",
    "with tarfile.open(dog_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=extract_path, filter='data')\n",
    "\n",
    "print(\"Archivos extraídos en:\", extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsdTCLC8uqzg"
   },
   "source": [
    "**Explorar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1762972282481,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "Y51SSRPbUk-a",
    "outputId": "aab81cd0-60a1-4c73-8e2e-9e1c23557d5d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "#Carpeta con las imágenes descomprimidas\n",
    "img_folder = '/content/dog'\n",
    "\n",
    "#Explorar subcarpetas (cada una representa un individuo)\n",
    "subfolders = [f.path for f in os.scandir(img_folder) if f.is_dir()]\n",
    "if not subfolders:\n",
    "    print(\"No se encontraron subcarpetas dentro de\", img_folder)\n",
    "else:\n",
    "    #Elegir una subcarpeta al azar\n",
    "    random_subfolder = random.choice(subfolders)\n",
    "\n",
    "    #Listar imágenes dentro de esa subcarpeta\n",
    "    images = [os.path.join(random_subfolder, f) for f in os.listdir(random_subfolder) if f.endswith('.png')]\n",
    "\n",
    "    if not images:\n",
    "        print(\"No se encontraron imágenes dentro de la subcarpeta:\", random_subfolder)\n",
    "    else:\n",
    "        #Elegir imágenes aleatorias\n",
    "        random_images = random.sample(images, min(5, len(images)))\n",
    "\n",
    "        #Mostrar imágenes\n",
    "        for img_path in random_images:\n",
    "            img = Image.open(img_path)\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(os.path.basename(img_path))\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 333618,
     "status": "ok",
     "timestamp": 1751997192811,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 240
    },
    "id": "IGdAC5bSGwe6",
    "outputId": "6032e213-f3bd-4929-ae04-471439b439ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/dog\n",
      "Loading the dataset...\n",
      "Done.\n",
      "Total number of imported pictures: 275068\n",
      "Total number of classes: 71613\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(img_folder)\n",
    "\n",
    "assert os.path.isdir(img_folder), '[Error] Provided PATH for dataset does not exist.'\n",
    "\n",
    "print('Loading the dataset...')\n",
    "\n",
    "filenames = np.empty(0)\n",
    "labels = np.empty(0)\n",
    "idx = 0\n",
    "for root,dirs,files in os.walk(img_folder):\n",
    "    if len(files)>1:\n",
    "        for i in range(len(files)):\n",
    "            files[i] = root + '/' + files[i]\n",
    "        filenames = np.append(filenames,files)\n",
    "        labels = np.append(labels,np.ones(len(files))*idx)\n",
    "        idx += 1\n",
    "assert len(labels)!=0, '[Error] No data provided.'\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "print('Total number of imported pictures: {:d}'.format(len(labels)))\n",
    "\n",
    "nbof_classes = len(np.unique(labels))\n",
    "print('Total number of classes: {:d}'.format(nbof_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1bGKFz5gPe"
   },
   "source": [
    "**Modelo MegaDescriptor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206,
     "referenced_widgets": [
      "940e932cdcaf45ca91d6d20093f6b5c4",
      "c461d5b0d787445eb77376e9c249b8b9",
      "e3fd6045c27c49d1b31a12c6d4a6186f",
      "6fdfda971e074b2c9c9be770b2c30384",
      "f95c2d649ea849809517abdbd362dd7f",
      "826a585f71734ec7a86245caf94abcba",
      "6f0bc18af0554becb81e407e5f98262d",
      "26213eebce274c658c64ffeafebb9231",
      "c23f946e321749709bbc09cf40e6e0e1",
      "a13766a47921412d8ee1ba0506c31b28",
      "212f06fe908f4dd59cff3d8092c957d2",
      "42bebbe916a14629b2950b6c79cca700",
      "2d8d40ac244a457b8149503cde78d1a5",
      "57ea6e43f5034366b9b07bf7a02e7da6",
      "88310afb7ddf40e7a35255cb9806e059",
      "89d7428bace041ef8edfbd4723cfdb28",
      "93cd394c8cd84802a2f9bfca3e593bfc",
      "16fc1513351e4e6ab8e710a3f2f66469",
      "f82add43fe24445e8b32a0c1c8ac0932",
      "08b4c271624b4799a34fce399b352bfa",
      "e37728e8d6c543ba992debb8f065b999",
      "4dcf99a566644f169fdb05aa32b63165"
     ]
    },
    "executionInfo": {
     "elapsed": 88829,
     "status": "ok",
     "timestamp": 1761572280135,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "uRKgcqUI4f3e",
    "outputId": "0d23ef9d-c07c-49c6-84f6-e7948c55d77b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940e932cdcaf45ca91d6d20093f6b5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bebbe916a14629b2950b6c79cca700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Carga el modelo directamente desde el Hub de Hugging Face\n",
    "# El modelo se descargará automáticamente\n",
    "model = timm.create_model(\n",
    "    'hf_hub:BVRA/MegaDescriptor-L-384', # Nombre del repositorio en el Hub\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "# Poner el modelo en modo de evaluación (importante para inferencia)\n",
    "model.eval()\n",
    "\n",
    "train_transforms = T.Compose([T.Resize(size=(384, 384)),\n",
    "                              T.ToTensor(),\n",
    "                              T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "print(\"Modelo cargado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtkhLO_dAzPA"
   },
   "source": [
    "**Prueba DINO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8051,
     "status": "ok",
     "timestamp": 1762972290535,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "5eVQ1ZhNBDO2",
    "outputId": "426b54fb-fb11-4469-f67c-0f748d1b9a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightly\n",
      "  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.10.5)\n",
      "Collecting hydra-core>=1.0.0 (from lightly)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lightly_utils~=0.0.0 (from lightly)\n",
      "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n",
      "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.23.0+cu126)\n",
      "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.11.10)\n",
      "Collecting pytorch_lightning>=1.0.4 (from lightly)\n",
      "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n",
      "Collecting aenum>=3.1.11 (from lightly)\n",
      "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n",
      "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n",
      "Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n",
      "Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
      "Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n",
      "Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.5.6 torchmetrics-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762972290538,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "44Il0ZSPAzAb"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "error",
     "timestamp": 1762972359559,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "Bdkurae5aylX",
    "outputId": "31170867-c7d1-4e00-852a-de9d7c1e9b94"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2007233871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfreeze_eval_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Freeze the parameters of a module.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def freeze_eval_module(module: Module) -> None:\n",
    "    \"\"\"Freeze the parameters of a module.\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "    module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10625,
     "status": "ok",
     "timestamp": 1762972430662,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "ERRJMdqsfb4i",
    "outputId": "6d4a425f-776e-4f30-98b4-9a24b222b3e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 275068 imágenes encontradas en /content/dog\n"
     ]
    }
   ],
   "source": [
    "import lightly.data\n",
    "# 2.3 - Definir el Dataset (apuntando a tu carpeta)\n",
    "path_dir = \"/content/dog\" # <--- ¡Asegúrate que esta sea tu ruta!\n",
    "try:\n",
    "    dataset = lightly.data.LightlyDataset(\n",
    "        input_dir=path_dir\n",
    "    )\n",
    "    print(f\"Dataset cargado: {len(dataset)} imágenes encontradas en {path_dir}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Carpeta '{path_dir}' no encontrada.\")\n",
    "    print(\"Asegúrate de descomprimir tu dataset primero (Celda 4 de tu notebook TBase1.ipynb).\")\n",
    "    # Salir o manejar el error apropiadamente si esto fuera un script .py\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33976312,
     "status": "ok",
     "timestamp": 1762553869052,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "2hyfT0Xn0Ikd",
    "outputId": "4714e11c-959d-4a0e-d69b-34f7f30f67db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: cuda, Batch size: 256\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 237MB/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 275068 imágenes encontradas.\n",
      "Iniciando entrenamiento con ResNet-50 (1074 steps por epoch)\n",
      "  Epoch: 00/20 | Step:     1/1074 | Loss: 8.57552 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   100/1074 | Loss: 9.00781 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   200/1074 | Loss: 9.00000 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   300/1074 | Loss: 8.97917 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   400/1074 | Loss: 8.92622 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   500/1074 | Loss: 8.79688 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   600/1074 | Loss: 8.41884 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   700/1074 | Loss: 7.64996 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   800/1074 | Loss: 6.82205 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:   900/1074 | Loss: 6.32335 | Momentum: 0.9960\n",
      "  Epoch: 00/20 | Step:  1000/1074 | Loss: 5.80729 | Momentum: 0.9960\n",
      "--- EPOCH 00 COMPLETE --- Avg Loss: 7.97027 ---\n",
      "  Epoch: 01/20 | Step:     1/1074 | Loss: 5.58116 | Momentum: 0.9960\n",
      "  Epoch: 01/20 | Step:   100/1074 | Loss: 5.46723 | Momentum: 0.9960\n",
      "  Epoch: 01/20 | Step:   200/1074 | Loss: 5.44379 | Momentum: 0.9960\n",
      "  Epoch: 01/20 | Step:   300/1074 | Loss: 5.22786 | Momentum: 0.9960\n",
      "  Epoch: 01/20 | Step:   400/1074 | Loss: 4.87522 | Momentum: 0.9960\n",
      "  Epoch: 01/20 | Step:   500/1074 | Loss: 4.83116 | Momentum: 0.9961\n",
      "  Epoch: 01/20 | Step:   600/1074 | Loss: 4.38911 | Momentum: 0.9961\n",
      "  Epoch: 01/20 | Step:   700/1074 | Loss: 4.43251 | Momentum: 0.9961\n",
      "  Epoch: 01/20 | Step:   800/1074 | Loss: 4.15972 | Momentum: 0.9961\n",
      "  Epoch: 01/20 | Step:   900/1074 | Loss: 4.09776 | Momentum: 0.9961\n",
      "  Epoch: 01/20 | Step:  1000/1074 | Loss: 3.97917 | Momentum: 0.9961\n",
      "--- EPOCH 01 COMPLETE --- Avg Loss: 4.65708 ---\n",
      "  Epoch: 02/20 | Step:     1/1074 | Loss: 3.75727 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   100/1074 | Loss: 3.63542 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   200/1074 | Loss: 3.68815 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   300/1074 | Loss: 3.65354 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   400/1074 | Loss: 3.50087 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   500/1074 | Loss: 3.37674 | Momentum: 0.9961\n",
      "  Epoch: 02/20 | Step:   600/1074 | Loss: 3.40853 | Momentum: 0.9962\n",
      "  Epoch: 02/20 | Step:   700/1074 | Loss: 3.15755 | Momentum: 0.9962\n",
      "  Epoch: 02/20 | Step:   800/1074 | Loss: 3.16678 | Momentum: 0.9962\n",
      "  Epoch: 02/20 | Step:   900/1074 | Loss: 3.14692 | Momentum: 0.9962\n",
      "  Epoch: 02/20 | Step:  1000/1074 | Loss: 3.12760 | Momentum: 0.9962\n",
      "--- EPOCH 02 COMPLETE --- Avg Loss: 3.40301 ---\n",
      "  Epoch: 03/20 | Step:     1/1074 | Loss: 2.78505 | Momentum: 0.9962\n",
      "  Epoch: 03/20 | Step:   100/1074 | Loss: 3.02018 | Momentum: 0.9962\n",
      "  Epoch: 03/20 | Step:   200/1074 | Loss: 2.92144 | Momentum: 0.9962\n",
      "  Epoch: 03/20 | Step:   300/1074 | Loss: 2.81326 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   400/1074 | Loss: 2.79644 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   500/1074 | Loss: 2.83529 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   600/1074 | Loss: 2.79232 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   700/1074 | Loss: 2.68294 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   800/1074 | Loss: 2.69282 | Momentum: 0.9963\n",
      "  Epoch: 03/20 | Step:   900/1074 | Loss: 2.63835 | Momentum: 0.9964\n",
      "  Epoch: 03/20 | Step:  1000/1074 | Loss: 2.58105 | Momentum: 0.9964\n",
      "--- EPOCH 03 COMPLETE --- Avg Loss: 2.80931 ---\n",
      "  Epoch: 04/20 | Step:     1/1074 | Loss: 2.52165 | Momentum: 0.9964\n",
      "  Epoch: 04/20 | Step:   100/1074 | Loss: 2.62901 | Momentum: 0.9964\n",
      "  Epoch: 04/20 | Step:   200/1074 | Loss: 2.67122 | Momentum: 0.9964\n",
      "  Epoch: 04/20 | Step:   300/1074 | Loss: 2.55762 | Momentum: 0.9964\n",
      "  Epoch: 04/20 | Step:   400/1074 | Loss: 2.43772 | Momentum: 0.9965\n",
      "  Epoch: 04/20 | Step:   500/1074 | Loss: 2.40717 | Momentum: 0.9965\n",
      "  Epoch: 04/20 | Step:   600/1074 | Loss: 2.46148 | Momentum: 0.9965\n",
      "  Epoch: 04/20 | Step:   700/1074 | Loss: 2.41477 | Momentum: 0.9965\n",
      "  Epoch: 04/20 | Step:   800/1074 | Loss: 2.46973 | Momentum: 0.9965\n",
      "  Epoch: 04/20 | Step:   900/1074 | Loss: 2.27463 | Momentum: 0.9966\n",
      "  Epoch: 04/20 | Step:  1000/1074 | Loss: 2.46929 | Momentum: 0.9966\n",
      "--- EPOCH 04 COMPLETE --- Avg Loss: 2.47301 ---\n",
      "  Epoch: 05/20 | Step:     1/1074 | Loss: 2.30897 | Momentum: 0.9966\n",
      "  Epoch: 05/20 | Step:   100/1074 | Loss: 2.47152 | Momentum: 0.9966\n",
      "  Epoch: 05/20 | Step:   200/1074 | Loss: 2.20736 | Momentum: 0.9966\n",
      "  Epoch: 05/20 | Step:   300/1074 | Loss: 2.29319 | Momentum: 0.9966\n",
      "  Epoch: 05/20 | Step:   400/1074 | Loss: 2.28564 | Momentum: 0.9967\n",
      "  Epoch: 05/20 | Step:   500/1074 | Loss: 2.30404 | Momentum: 0.9967\n",
      "  Epoch: 05/20 | Step:   600/1074 | Loss: 2.32796 | Momentum: 0.9967\n",
      "  Epoch: 05/20 | Step:   700/1074 | Loss: 2.18077 | Momentum: 0.9967\n",
      "  Epoch: 05/20 | Step:   800/1074 | Loss: 2.33404 | Momentum: 0.9968\n",
      "  Epoch: 05/20 | Step:   900/1074 | Loss: 2.19857 | Momentum: 0.9968\n",
      "  Epoch: 05/20 | Step:  1000/1074 | Loss: 2.07959 | Momentum: 0.9968\n",
      "--- EPOCH 05 COMPLETE --- Avg Loss: 2.27299 ---\n",
      "  Epoch: 06/20 | Step:     1/1074 | Loss: 2.22890 | Momentum: 0.9968\n",
      "  Epoch: 06/20 | Step:   100/1074 | Loss: 2.19135 | Momentum: 0.9968\n",
      "  Epoch: 06/20 | Step:   200/1074 | Loss: 2.17969 | Momentum: 0.9969\n",
      "  Epoch: 06/20 | Step:   300/1074 | Loss: 2.11464 | Momentum: 0.9969\n",
      "  Epoch: 06/20 | Step:   400/1074 | Loss: 2.02566 | Momentum: 0.9969\n",
      "  Epoch: 06/20 | Step:   500/1074 | Loss: 2.10314 | Momentum: 0.9969\n",
      "  Epoch: 06/20 | Step:   600/1074 | Loss: 2.16406 | Momentum: 0.9970\n",
      "  Epoch: 06/20 | Step:   700/1074 | Loss: 2.21278 | Momentum: 0.9970\n",
      "  Epoch: 06/20 | Step:   800/1074 | Loss: 2.04514 | Momentum: 0.9970\n",
      "  Epoch: 06/20 | Step:   900/1074 | Loss: 2.24929 | Momentum: 0.9970\n",
      "  Epoch: 06/20 | Step:  1000/1074 | Loss: 2.12348 | Momentum: 0.9971\n",
      "--- EPOCH 06 COMPLETE --- Avg Loss: 2.13191 ---\n",
      "  Epoch: 07/20 | Step:     1/1074 | Loss: 2.13829 | Momentum: 0.9971\n",
      "  Epoch: 07/20 | Step:   100/1074 | Loss: 2.10520 | Momentum: 0.9971\n",
      "  Epoch: 07/20 | Step:   200/1074 | Loss: 2.15435 | Momentum: 0.9971\n",
      "  Epoch: 07/20 | Step:   300/1074 | Loss: 2.05165 | Momentum: 0.9972\n",
      "  Epoch: 07/20 | Step:   400/1074 | Loss: 1.93701 | Momentum: 0.9972\n",
      "  Epoch: 07/20 | Step:   500/1074 | Loss: 1.95144 | Momentum: 0.9972\n",
      "  Epoch: 07/20 | Step:   600/1074 | Loss: 2.03196 | Momentum: 0.9973\n",
      "  Epoch: 07/20 | Step:   700/1074 | Loss: 1.98459 | Momentum: 0.9973\n",
      "  Epoch: 07/20 | Step:   800/1074 | Loss: 1.94032 | Momentum: 0.9973\n",
      "  Epoch: 07/20 | Step:   900/1074 | Loss: 1.94548 | Momentum: 0.9973\n",
      "  Epoch: 07/20 | Step:  1000/1074 | Loss: 1.92497 | Momentum: 0.9974\n",
      "--- EPOCH 07 COMPLETE --- Avg Loss: 2.02861 ---\n",
      "  Epoch: 08/20 | Step:     1/1074 | Loss: 2.02799 | Momentum: 0.9974\n",
      "  Epoch: 08/20 | Step:   100/1074 | Loss: 1.99094 | Momentum: 0.9974\n",
      "  Epoch: 08/20 | Step:   200/1074 | Loss: 1.95920 | Momentum: 0.9974\n",
      "  Epoch: 08/20 | Step:   300/1074 | Loss: 1.88900 | Momentum: 0.9975\n",
      "  Epoch: 08/20 | Step:   400/1074 | Loss: 1.89475 | Momentum: 0.9975\n",
      "  Epoch: 08/20 | Step:   500/1074 | Loss: 1.88509 | Momentum: 0.9975\n",
      "  Epoch: 08/20 | Step:   600/1074 | Loss: 1.86464 | Momentum: 0.9976\n",
      "  Epoch: 08/20 | Step:   700/1074 | Loss: 1.97119 | Momentum: 0.9976\n",
      "  Epoch: 08/20 | Step:   800/1074 | Loss: 1.91732 | Momentum: 0.9976\n",
      "  Epoch: 08/20 | Step:   900/1074 | Loss: 1.93001 | Momentum: 0.9976\n",
      "  Epoch: 08/20 | Step:  1000/1074 | Loss: 2.00228 | Momentum: 0.9977\n",
      "--- EPOCH 08 COMPLETE --- Avg Loss: 1.94608 ---\n",
      "  Epoch: 09/20 | Step:     1/1074 | Loss: 1.89095 | Momentum: 0.9977\n",
      "  Epoch: 09/20 | Step:   100/1074 | Loss: 1.89415 | Momentum: 0.9977\n",
      "  Epoch: 09/20 | Step:   200/1074 | Loss: 1.84755 | Momentum: 0.9977\n",
      "  Epoch: 09/20 | Step:   300/1074 | Loss: 1.75613 | Momentum: 0.9978\n",
      "  Epoch: 09/20 | Step:   400/1074 | Loss: 1.86334 | Momentum: 0.9978\n",
      "  Epoch: 09/20 | Step:   500/1074 | Loss: 1.76101 | Momentum: 0.9978\n",
      "  Epoch: 09/20 | Step:   600/1074 | Loss: 1.86361 | Momentum: 0.9979\n",
      "  Epoch: 09/20 | Step:   700/1074 | Loss: 1.89572 | Momentum: 0.9979\n",
      "  Epoch: 09/20 | Step:   800/1074 | Loss: 1.96571 | Momentum: 0.9979\n",
      "  Epoch: 09/20 | Step:   900/1074 | Loss: 1.94916 | Momentum: 0.9979\n",
      "  Epoch: 09/20 | Step:  1000/1074 | Loss: 1.88243 | Momentum: 0.9980\n",
      "--- EPOCH 09 COMPLETE --- Avg Loss: 1.87675 ---\n",
      "  Epoch: 10/20 | Step:     1/1074 | Loss: 1.78277 | Momentum: 0.9980\n",
      "  Epoch: 10/20 | Step:   100/1074 | Loss: 1.79530 | Momentum: 0.9980\n",
      "  Epoch: 10/20 | Step:   200/1074 | Loss: 1.77268 | Momentum: 0.9981\n",
      "  Epoch: 10/20 | Step:   300/1074 | Loss: 1.85579 | Momentum: 0.9981\n",
      "  Epoch: 10/20 | Step:   400/1074 | Loss: 1.87240 | Momentum: 0.9981\n",
      "  Epoch: 10/20 | Step:   500/1074 | Loss: 1.80518 | Momentum: 0.9981\n",
      "  Epoch: 10/20 | Step:   600/1074 | Loss: 1.87717 | Momentum: 0.9982\n",
      "  Epoch: 10/20 | Step:   700/1074 | Loss: 1.77599 | Momentum: 0.9982\n",
      "  Epoch: 10/20 | Step:   800/1074 | Loss: 1.80143 | Momentum: 0.9982\n",
      "  Epoch: 10/20 | Step:   900/1074 | Loss: 1.82899 | Momentum: 0.9983\n",
      "  Epoch: 10/20 | Step:  1000/1074 | Loss: 1.75939 | Momentum: 0.9983\n",
      "--- EPOCH 10 COMPLETE --- Avg Loss: 1.82367 ---\n",
      "  Epoch: 11/20 | Step:     1/1074 | Loss: 1.82498 | Momentum: 0.9983\n",
      "  Epoch: 11/20 | Step:   100/1074 | Loss: 1.70752 | Momentum: 0.9983\n",
      "  Epoch: 11/20 | Step:   200/1074 | Loss: 1.84739 | Momentum: 0.9984\n",
      "  Epoch: 11/20 | Step:   300/1074 | Loss: 1.78315 | Momentum: 0.9984\n",
      "  Epoch: 11/20 | Step:   400/1074 | Loss: 1.73432 | Momentum: 0.9984\n",
      "  Epoch: 11/20 | Step:   500/1074 | Loss: 1.82433 | Momentum: 0.9985\n",
      "  Epoch: 11/20 | Step:   600/1074 | Loss: 1.84283 | Momentum: 0.9985\n",
      "  Epoch: 11/20 | Step:   700/1074 | Loss: 1.79107 | Momentum: 0.9985\n",
      "  Epoch: 11/20 | Step:   800/1074 | Loss: 1.69005 | Momentum: 0.9985\n",
      "  Epoch: 11/20 | Step:   900/1074 | Loss: 1.72597 | Momentum: 0.9986\n",
      "  Epoch: 11/20 | Step:  1000/1074 | Loss: 1.81424 | Momentum: 0.9986\n",
      "--- EPOCH 11 COMPLETE --- Avg Loss: 1.78088 ---\n",
      "  Epoch: 12/20 | Step:     1/1074 | Loss: 1.75255 | Momentum: 0.9986\n",
      "  Epoch: 12/20 | Step:   100/1074 | Loss: 1.63504 | Momentum: 0.9986\n",
      "  Epoch: 12/20 | Step:   200/1074 | Loss: 1.79384 | Momentum: 0.9987\n",
      "  Epoch: 12/20 | Step:   300/1074 | Loss: 1.67008 | Momentum: 0.9987\n",
      "  Epoch: 12/20 | Step:   400/1074 | Loss: 1.72786 | Momentum: 0.9987\n",
      "  Epoch: 12/20 | Step:   500/1074 | Loss: 1.77105 | Momentum: 0.9988\n",
      "  Epoch: 12/20 | Step:   600/1074 | Loss: 1.68376 | Momentum: 0.9988\n",
      "  Epoch: 12/20 | Step:   700/1074 | Loss: 1.75049 | Momentum: 0.9988\n",
      "  Epoch: 12/20 | Step:   800/1074 | Loss: 1.67269 | Momentum: 0.9988\n",
      "  Epoch: 12/20 | Step:   900/1074 | Loss: 1.71088 | Momentum: 0.9989\n",
      "  Epoch: 12/20 | Step:  1000/1074 | Loss: 1.76975 | Momentum: 0.9989\n",
      "--- EPOCH 12 COMPLETE --- Avg Loss: 1.74190 ---\n",
      "  Epoch: 13/20 | Step:     1/1074 | Loss: 1.67339 | Momentum: 0.9989\n",
      "  Epoch: 13/20 | Step:   100/1074 | Loss: 1.67448 | Momentum: 0.9989\n",
      "  Epoch: 13/20 | Step:   200/1074 | Loss: 1.64979 | Momentum: 0.9990\n",
      "  Epoch: 13/20 | Step:   300/1074 | Loss: 1.73600 | Momentum: 0.9990\n",
      "  Epoch: 13/20 | Step:   400/1074 | Loss: 1.70177 | Momentum: 0.9990\n",
      "  Epoch: 13/20 | Step:   500/1074 | Loss: 1.65956 | Momentum: 0.9990\n",
      "  Epoch: 13/20 | Step:   600/1074 | Loss: 1.67220 | Momentum: 0.9991\n",
      "  Epoch: 13/20 | Step:   700/1074 | Loss: 1.65034 | Momentum: 0.9991\n",
      "  Epoch: 13/20 | Step:   800/1074 | Loss: 1.74029 | Momentum: 0.9991\n",
      "  Epoch: 13/20 | Step:   900/1074 | Loss: 1.80713 | Momentum: 0.9991\n",
      "  Epoch: 13/20 | Step:  1000/1074 | Loss: 1.68256 | Momentum: 0.9992\n",
      "--- EPOCH 13 COMPLETE --- Avg Loss: 1.70849 ---\n",
      "  Epoch: 14/20 | Step:     1/1074 | Loss: 1.66721 | Momentum: 0.9992\n",
      "  Epoch: 14/20 | Step:   100/1074 | Loss: 1.71962 | Momentum: 0.9992\n",
      "  Epoch: 14/20 | Step:   200/1074 | Loss: 1.69173 | Momentum: 0.9992\n",
      "  Epoch: 14/20 | Step:   300/1074 | Loss: 1.74582 | Momentum: 0.9992\n",
      "  Epoch: 14/20 | Step:   400/1074 | Loss: 1.60520 | Momentum: 0.9993\n",
      "  Epoch: 14/20 | Step:   500/1074 | Loss: 1.72900 | Momentum: 0.9993\n",
      "  Epoch: 14/20 | Step:   600/1074 | Loss: 1.67307 | Momentum: 0.9993\n",
      "  Epoch: 14/20 | Step:   700/1074 | Loss: 1.63178 | Momentum: 0.9993\n",
      "  Epoch: 14/20 | Step:   800/1074 | Loss: 1.59755 | Momentum: 0.9994\n",
      "  Epoch: 14/20 | Step:   900/1074 | Loss: 1.75662 | Momentum: 0.9994\n",
      "  Epoch: 14/20 | Step:  1000/1074 | Loss: 1.64594 | Momentum: 0.9994\n",
      "--- EPOCH 14 COMPLETE --- Avg Loss: 1.68147 ---\n",
      "  Epoch: 15/20 | Step:     1/1074 | Loss: 1.72721 | Momentum: 0.9994\n",
      "  Epoch: 15/20 | Step:   100/1074 | Loss: 1.62077 | Momentum: 0.9994\n",
      "  Epoch: 15/20 | Step:   200/1074 | Loss: 1.66390 | Momentum: 0.9995\n",
      "  Epoch: 15/20 | Step:   300/1074 | Loss: 1.73454 | Momentum: 0.9995\n",
      "  Epoch: 15/20 | Step:   400/1074 | Loss: 1.70372 | Momentum: 0.9995\n",
      "  Epoch: 15/20 | Step:   500/1074 | Loss: 1.64334 | Momentum: 0.9995\n",
      "  Epoch: 15/20 | Step:   600/1074 | Loss: 1.61268 | Momentum: 0.9995\n",
      "  Epoch: 15/20 | Step:   700/1074 | Loss: 1.63845 | Momentum: 0.9996\n",
      "  Epoch: 15/20 | Step:   800/1074 | Loss: 1.66477 | Momentum: 0.9996\n",
      "  Epoch: 15/20 | Step:   900/1074 | Loss: 1.52813 | Momentum: 0.9996\n",
      "  Epoch: 15/20 | Step:  1000/1074 | Loss: 1.58048 | Momentum: 0.9996\n",
      "--- EPOCH 15 COMPLETE --- Avg Loss: 1.65800 ---\n",
      "  Epoch: 16/20 | Step:     1/1074 | Loss: 1.62847 | Momentum: 0.9996\n",
      "  Epoch: 16/20 | Step:   100/1074 | Loss: 1.56616 | Momentum: 0.9996\n",
      "  Epoch: 16/20 | Step:   200/1074 | Loss: 1.60232 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   300/1074 | Loss: 1.73128 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   400/1074 | Loss: 1.72309 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   500/1074 | Loss: 1.57647 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   600/1074 | Loss: 1.53073 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   700/1074 | Loss: 1.66151 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   800/1074 | Loss: 1.68338 | Momentum: 0.9997\n",
      "  Epoch: 16/20 | Step:   900/1074 | Loss: 1.59836 | Momentum: 0.9998\n",
      "  Epoch: 16/20 | Step:  1000/1074 | Loss: 1.62391 | Momentum: 0.9998\n",
      "--- EPOCH 16 COMPLETE --- Avg Loss: 1.63722 ---\n",
      "  Epoch: 17/20 | Step:     1/1074 | Loss: 1.53909 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   100/1074 | Loss: 1.64187 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   200/1074 | Loss: 1.68186 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   300/1074 | Loss: 1.60297 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   400/1074 | Loss: 1.63911 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   500/1074 | Loss: 1.62025 | Momentum: 0.9998\n",
      "  Epoch: 17/20 | Step:   600/1074 | Loss: 1.64350 | Momentum: 0.9999\n",
      "  Epoch: 17/20 | Step:   700/1074 | Loss: 1.61561 | Momentum: 0.9999\n",
      "  Epoch: 17/20 | Step:   800/1074 | Loss: 1.67985 | Momentum: 0.9999\n",
      "  Epoch: 17/20 | Step:   900/1074 | Loss: 1.59972 | Momentum: 0.9999\n",
      "  Epoch: 17/20 | Step:  1000/1074 | Loss: 1.73063 | Momentum: 0.9999\n",
      "--- EPOCH 17 COMPLETE --- Avg Loss: 1.62047 ---\n",
      "  Epoch: 18/20 | Step:     1/1074 | Loss: 1.66634 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   100/1074 | Loss: 1.65386 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   200/1074 | Loss: 1.70367 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   300/1074 | Loss: 1.59451 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   400/1074 | Loss: 1.61309 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   500/1074 | Loss: 1.58697 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   600/1074 | Loss: 1.60319 | Momentum: 0.9999\n",
      "  Epoch: 18/20 | Step:   700/1074 | Loss: 1.57137 | Momentum: 1.0000\n",
      "  Epoch: 18/20 | Step:   800/1074 | Loss: 1.55781 | Momentum: 1.0000\n",
      "  Epoch: 18/20 | Step:   900/1074 | Loss: 1.65609 | Momentum: 1.0000\n",
      "  Epoch: 18/20 | Step:  1000/1074 | Loss: 1.57416 | Momentum: 1.0000\n",
      "--- EPOCH 18 COMPLETE --- Avg Loss: 1.60728 ---\n",
      "  Epoch: 19/20 | Step:     1/1074 | Loss: 1.56950 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   100/1074 | Loss: 1.60026 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   200/1074 | Loss: 1.57525 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   300/1074 | Loss: 1.63314 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   400/1074 | Loss: 1.57883 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   500/1074 | Loss: 1.63957 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   600/1074 | Loss: 1.61583 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   700/1074 | Loss: 1.60270 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   800/1074 | Loss: 1.68739 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:   900/1074 | Loss: 1.78684 | Momentum: 1.0000\n",
      "  Epoch: 19/20 | Step:  1000/1074 | Loss: 1.57818 | Momentum: 1.0000\n",
      "--- EPOCH 19 COMPLETE --- Avg Loss: 1.59463 ---\n",
      "Entrenamiento de prueba (ResNet-50) finalizado.\n",
      "Modelo backbone guardado en 'dino_resnet50_petface_backbone.pth'\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lightly.data\n",
    "from lightly.data.collate import DINOCollateFunction\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.utils.scheduler import cosine_schedule, linear_warmup_schedule\n",
    "from lightly.models.utils import update_momentum\n",
    "import os\n",
    "\n",
    "# --- 1. Parámetros de Entrenamiento ---\n",
    "# ⚠️ BATCH_SIZE: 64 debería funcionar bien en una T4/V100 con ResNet-50\n",
    "# Súbelo a 128 o 256 si tienes una A100 (40GB)\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 20 # 20 épocas para esta prueba\n",
    "LR = 0.001\n",
    "N_LOCAL_VIEWS = 8 # ResNet es rápido, podemos usar 8\n",
    "DATASET_PATH = \"/content/dog\"\n",
    "\n",
    "print(f\"Usando device: {DEVICE}, Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- 2. Modelo (Student y Teacher con ResNet-50) ---\n",
    "\n",
    "weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "backbone = torchvision.models.resnet50(weights=weights)\n",
    "# Obtenemos la dimensión de salida (2048)\n",
    "backbone_dim = backbone.fc.in_features\n",
    "# Reemplazamos la capa de clasificación\n",
    "backbone.fc = nn.Identity()\n",
    "\n",
    "# Definimos el DINOProjectionHead (más ligero que el de DINOv2)\n",
    "model_head = DINOProjectionHead(\n",
    "    input_dim=backbone_dim,\n",
    "    hidden_dim=2048,\n",
    "    output_dim=8192 # Un output_dim estándar para DINO\n",
    ")\n",
    "\n",
    "# Definimos los argumentos para nuestra cabeza de proyección\n",
    "head_args = {\n",
    "    \"input_dim\": backbone_dim,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"output_dim\": 8192\n",
    "}\n",
    "\n",
    "# 1. Crear el backbone del student y del teacher\n",
    "student_backbone = backbone.to(DEVICE)\n",
    "teacher_backbone = copy.deepcopy(backbone).to(DEVICE) # deepcopy está bien para ResNet\n",
    "\n",
    "# 2. Crear la cabeza del student\n",
    "student_head = DINOProjectionHead(**head_args).to(DEVICE)\n",
    "\n",
    "# 3. Crear una NUEVA instancia de la cabeza del teacher\n",
    "teacher_head = DINOProjectionHead(**head_args)\n",
    "\n",
    "# 4. Copiar los pesos (state_dict) del student al teacher\n",
    "teacher_head.load_state_dict(student_head.state_dict())\n",
    "\n",
    "# 5. Mover el teacher a la GPU\n",
    "teacher_head = teacher_head.to(DEVICE)\n",
    "\n",
    "# Congelamos el teacher\n",
    "for param in teacher_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in teacher_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- 3. Dataloader ---\n",
    "collate_fn = DINOCollateFunction(\n",
    "    global_crop_size=224,\n",
    "    local_crop_size=96,\n",
    "    n_local_views=N_LOCAL_VIEWS,\n",
    ")\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Carpeta '{DATASET_PATH}' no encontrada.\")\n",
    "else:\n",
    "    dataset = lightly.data.LightlyDataset(input_dir=DATASET_PATH)\n",
    "    print(f\"Dataset cargado: {len(dataset)} imágenes encontradas.\")\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # --- 4. Pérdida, Optimizador, AMP ---\n",
    "    criterion = DINOLoss(\n",
    "        output_dim=8192,\n",
    "        warmup_teacher_temp_epochs=5\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    params_student = list(student_backbone.parameters()) + list(student_head.parameters())\n",
    "    optimizer = AdamW(params_student, lr=LR)\n",
    "\n",
    "    scaler = torch.amp.GradScaler() # AMP\n",
    "    num_batches = len(dataloader)\n",
    "    total_steps = EPOCHS * num_batches\n",
    "    momentum_scheduler = cosine_schedule(0, EPOCHS, 0.996, 1.0)\n",
    "\n",
    "    # --- 5. 🚀 Loop de Entrenamiento (Corregido) ---\n",
    "print(f\"Iniciando entrenamiento con ResNet-50 ({num_batches} steps por epoch)\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # ⚠️ NO calculamos momentum aquí fuera del loop\n",
    "\n",
    "    for batch_idx, (views, _, _) in enumerate(dataloader):\n",
    "\n",
    "        # --- CAMBIO 1: Calcular momentum y step global AQUÍ ---\n",
    "        global_step = epoch * num_batches + batch_idx\n",
    "        m = cosine_schedule(\n",
    "            step=global_step,\n",
    "            max_steps=total_steps,\n",
    "            start_value=0.996,\n",
    "            end_value=1.0\n",
    "        )\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        views = [v.to(DEVICE) for v in views]\n",
    "        global_views = views[:2]\n",
    "        local_views = views[2:]\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            # Teacher (sin gradientes)\n",
    "            teacher_out = []\n",
    "            with torch.no_grad():\n",
    "                for v in global_views:\n",
    "                    features = teacher_backbone(v)\n",
    "                    out = teacher_head(features)\n",
    "                    teacher_out.append(out)\n",
    "\n",
    "            # Student (con gradientes)\n",
    "            student_out = []\n",
    "            for v in global_views + local_views:\n",
    "                features = student_backbone(v)\n",
    "                out = student_head(features)\n",
    "                student_out.append(out)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # --- CAMBIO 2: Mover la actualización del Teacher AQUÍ (al final del step) ---\n",
    "        update_momentum(student_backbone, teacher_backbone, m=m)\n",
    "        update_momentum(student_head, teacher_head, m=m)\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0 or batch_idx == 0:\n",
    "            # Añadimos 'm' al log para ver cómo cambia\n",
    "            print(f\"  Epoch: {epoch:>02}/{EPOCHS} | Step: {batch_idx+1:>5}/{num_batches} | Loss: {loss.item():.5f} | Momentum: {m:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"--- EPOCH {epoch:>02} COMPLETE --- Avg Loss: {avg_loss:.5f} ---\")\n",
    "\n",
    "print(\"Entrenamiento de prueba (ResNet-50) finalizado.\")\n",
    "torch.save(student_backbone.state_dict(), 'dino_resnet50_petface_backbone.pth')\n",
    "print(\"Modelo backbone guardado en 'dino_resnet50_petface_backbone.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg6fSljfRUou"
   },
   "source": [
    "**Evaluación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 38800,
     "status": "error",
     "timestamp": 1762972743271,
     "user": {
      "displayName": "RJ RL",
      "userId": "07876038978205557392"
     },
     "user_tz": 180
    },
    "id": "0044Aga2RSQe",
    "outputId": "c33dcc81-9c31-4f48-a4c3-d8cac801f1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Total de imágenes: 46755 | Individuos: 46755\n",
      "📊 Generado split automático: {'gallery': 46755}\n",
      "🔹 Cargando modelo DINO-ResNet50...\n",
      "✅ Modelo cargado correctamente.\n",
      "🔹 Extrayendo embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [00:36<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generados: (46755, 2048)\n",
      "📂 Query: 0 | Gallery: 46755\n",
      "🔹 Calculando matriz de similitud...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2048)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1598728611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# CALCULAR SIMILITUD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔹 Calculando matriz de similitud...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# FUNCIÓN mAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n\u001b[1;32m    199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         X = check_array(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2048)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "\n",
    "# Configuración\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BACKBONE_PATH = \"/content/drive/MyDrive/Backbones/dino_resnet50_petface_backbone.pth\"\n",
    "CSV_PATH = \"/content/drive/MyDrive/PetFace/split/dog/reidentification.csv\"\n",
    "\n",
    "# Leer CSV y crear split\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if not {\"filename\", \"label\"}.issubset(df.columns):\n",
    "    raise ValueError(\"El CSV debe contener las columnas: filename,label\")\n",
    "\n",
    "print(f\"📁 Total de imágenes: {len(df)} | Individuos: {df['label'].nunique()}\")\n",
    "\n",
    "# Crear columnas de split: 1 query por clase\n",
    "splits = []\n",
    "for label, group in df.groupby(\"label\"):\n",
    "    paths = group[\"filename\"].tolist()\n",
    "    if len(paths) < 2:\n",
    "        for _ in paths:\n",
    "            splits.append(\"gallery\")\n",
    "    else:\n",
    "        q = random.choice(paths)\n",
    "        for p in paths:\n",
    "            splits.append(\"query\" if p == q else \"gallery\")\n",
    "df[\"split\"] = splits\n",
    "\n",
    "print(f\"📊 Generado split automático: {df['split'].value_counts().to_dict()}\")\n",
    "\n",
    "# Caegar Modelo\n",
    "print(\"🔹 Cargando modelo DINO-ResNet50...\")\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Identity()\n",
    "model.load_state_dict(torch.load(BACKBONE_PATH, map_location=DEVICE))\n",
    "model.eval().to(DEVICE)\n",
    "print(\"✅ Modelo cargado correctamente.\")\n",
    "\n",
    "# Transformaciones\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# dataset y dataloader\n",
    "class DogDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.paths = df[\"filename\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"No se encontró la imagen: {path}\")\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(img), self.labels[idx]\n",
    "\n",
    "dataset = DogDataset(df, transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "# Extraer Embeddings\n",
    "print(\"🔹 Extrayendo embeddings...\")\n",
    "embeddings, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in tqdm(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        feats = model(imgs)\n",
    "        feats = nn.functional.normalize(feats, dim=1)\n",
    "        embeddings.append(feats.cpu().numpy())\n",
    "        labels.extend(labs)\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.array(labels)\n",
    "print(f\"✅ Embeddings generados: {embeddings.shape}\")\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ SEPARAR QUERY / GALLERY\n",
    "# ============================\n",
    "query_mask = df[\"split\"] == \"query\"\n",
    "gallery_mask = df[\"split\"] == \"gallery\"\n",
    "\n",
    "query_emb = embeddings[query_mask]\n",
    "gallery_emb = embeddings[gallery_mask]\n",
    "query_labels = labels[query_mask]\n",
    "gallery_labels = labels[gallery_mask]\n",
    "\n",
    "print(f\"📂 Query: {len(query_emb)} | Gallery: {len(gallery_emb)}\")\n",
    "\n",
    "# CALCULAR SIMILITUD\n",
    "print(\"🔹 Calculando matriz de similitud...\")\n",
    "sim_matrix = cosine_similarity(query_emb, gallery_emb)\n",
    "\n",
    "# FUNCIÓN mAP\n",
    "def average_precision(sim_row, query_label, gallery_labels):\n",
    "    sorted_idx = np.argsort(-sim_row)\n",
    "    sorted_labels = gallery_labels[sorted_idx]\n",
    "    correct = (sorted_labels == query_label).astype(int)\n",
    "    cum_correct = np.cumsum(correct)\n",
    "    precision = cum_correct / (np.arange(len(correct)) + 1)\n",
    "    return np.sum(precision * correct) / max(np.sum(correct), 1)\n",
    "\n",
    "print(\"🔹 Calculando mAP...\")\n",
    "mAP = np.mean([\n",
    "    average_precision(sim_matrix[i], query_labels[i], gallery_labels)\n",
    "    for i in range(len(query_emb))\n",
    "])\n",
    "\n",
    "# k-NN Accuracy (Top-1)\n",
    "knn = KNeighborsClassifier(n_neighbors=200, metric=\"cosine\")\n",
    "knn.fit(gallery_emb, gallery_labels)\n",
    "acc = knn.score(query_emb, query_labels)\n",
    "\n",
    "# RESULTADOS\n",
    "print(\"\\n📊 RESULTADOS DE RE-IDENTIFICACIÓN SSL\")\n",
    "print(f\"mean Average Precision (mAP): {mAP:.4f}\")\n",
    "print(f\"k-NN Top-1 Accuracy (k=200): {acc:.4f}\")\n",
    "print(\"✅ Evaluación completada con éxito.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM5YSvuFcGFA2LEvGkqdvKN",
   "machine_shape": "hm",
   "mount_file_id": "17bxLS5CLEhyWRBCasAGtcz2p3Z1GgcFa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08b4c271624b4799a34fce399b352bfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16fc1513351e4e6ab8e710a3f2f66469": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "212f06fe908f4dd59cff3d8092c957d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26213eebce274c658c64ffeafebb9231": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d8d40ac244a457b8149503cde78d1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93cd394c8cd84802a2f9bfca3e593bfc",
      "placeholder": "​",
      "style": "IPY_MODEL_16fc1513351e4e6ab8e710a3f2f66469",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "42bebbe916a14629b2950b6c79cca700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d8d40ac244a457b8149503cde78d1a5",
       "IPY_MODEL_57ea6e43f5034366b9b07bf7a02e7da6",
       "IPY_MODEL_88310afb7ddf40e7a35255cb9806e059"
      ],
      "layout": "IPY_MODEL_89d7428bace041ef8edfbd4723cfdb28"
     }
    },
    "4dcf99a566644f169fdb05aa32b63165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ea6e43f5034366b9b07bf7a02e7da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f82add43fe24445e8b32a0c1c8ac0932",
      "max": 1935177938,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08b4c271624b4799a34fce399b352bfa",
      "value": 1935177938
     }
    },
    "6f0bc18af0554becb81e407e5f98262d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fdfda971e074b2c9c9be770b2c30384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a13766a47921412d8ee1ba0506c31b28",
      "placeholder": "​",
      "style": "IPY_MODEL_212f06fe908f4dd59cff3d8092c957d2",
      "value": " 609/609 [00:00&lt;00:00, 59.4kB/s]"
     }
    },
    "826a585f71734ec7a86245caf94abcba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88310afb7ddf40e7a35255cb9806e059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e37728e8d6c543ba992debb8f065b999",
      "placeholder": "​",
      "style": "IPY_MODEL_4dcf99a566644f169fdb05aa32b63165",
      "value": " 1.94G/1.94G [01:08&lt;00:00, 23.7MB/s]"
     }
    },
    "89d7428bace041ef8edfbd4723cfdb28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93cd394c8cd84802a2f9bfca3e593bfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "940e932cdcaf45ca91d6d20093f6b5c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c461d5b0d787445eb77376e9c249b8b9",
       "IPY_MODEL_e3fd6045c27c49d1b31a12c6d4a6186f",
       "IPY_MODEL_6fdfda971e074b2c9c9be770b2c30384"
      ],
      "layout": "IPY_MODEL_f95c2d649ea849809517abdbd362dd7f"
     }
    },
    "a13766a47921412d8ee1ba0506c31b28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c23f946e321749709bbc09cf40e6e0e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c461d5b0d787445eb77376e9c249b8b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_826a585f71734ec7a86245caf94abcba",
      "placeholder": "​",
      "style": "IPY_MODEL_6f0bc18af0554becb81e407e5f98262d",
      "value": "config.json: 100%"
     }
    },
    "e37728e8d6c543ba992debb8f065b999": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3fd6045c27c49d1b31a12c6d4a6186f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26213eebce274c658c64ffeafebb9231",
      "max": 609,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c23f946e321749709bbc09cf40e6e0e1",
      "value": 609
     }
    },
    "f82add43fe24445e8b32a0c1c8ac0932": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f95c2d649ea849809517abdbd362dd7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
